---
header-includes:
- \providecommand{\titletxt}{Tutorial - Spatial Analysis in R with Open Geodata}
- \usepackage{listings}
- \usepackage[skins]{tcolorbox}
- \usepackage{float}
- \usepackage{multicol}
- \usepackage{graphicx}
- \usepackage{lastpage}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \lhead{Use of R in Official Statistics - uRos2018}
- \rhead{6th international conference, The Hague, September 2018}
- \usepackage[numbered]{bookmark}
- \usepackage{hyperref}
- \hypersetup{ pdftitle={Tutorial - Spatial Analysis in R with Open Geodata - uRos2018},
  pdfauthor={Egge-Jan PollÃ© and Willy Tadema} }
- \hypersetup{linkcolor = {blue}, urlcolor = {blue}}
- \renewcommand{\footrulewidth}{0.4pt}
- \fancyfoot{}
- \fancyfoot[l]{\titletxt}
- \fancyfoot[r]{Page \thepage\ of \pageref*{LastPage}}
- \usepackage{fvextra}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
- \usepackage{xcolor}
- \definecolor{grey}{rgb}{0.8,0.8,0.8}
- \usepackage{enumitem}
- \renewcommand{\figurename}{Figure}
mainfont: Calibri
output:
  pdf_document:
    latex_engine: lualatex
  html_document:
    df_print: paged
urlcolor: blue
---


\newpage

```{r setup, include=FALSE, cache = FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, strip.white = FALSE, echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
```

\huge Tutorial - Spatial Analysis in R with Open Geodata

\normalsize

**Egge-Jan Pollé** - Tensing GIS Consultancy B.V.[^1]  
**Willy Tadema** - Provincie Groningen[^2]

Version 0.4.5 - August 20, 2018

\begin{multicols}{2}

\subsection{Introduction}


This is the manual for a tutorial session to be delivered on Wednesday 12 September 2018 at the 6\textsuperscript{th} International \href{https://www.aanmelder.nl/uros2018}{Conference on the Use of R in Official Statistics (uRos2018)} at the Dutch Office for National Statistics (\href{https://www.cbs.nl/en-gb}{CBS}) in the Hague.

\begin{itemize}
\item Date: \textbf{Wednesday, September 12, 2018}  
\item Time: \textbf{9:00 - 12:30}
\item Location: \textbf{Tinbergenzaal} (at CBS, Henri Faasdreef 312, 2492JP The Hague, The Netherlands)
\end{itemize}

\subsection{Prerequisites}

\begin{itemize}
  \item Attendees should bring their own laptop with (64-bit) \href{https://www.r-project.org/}{R}, and preferably also \href{https://www.rstudio.com/}{RStudio}, installed.
  \item Shortly before the training session we will publish a list of additional packages (e.g. \textbf{sf}, \textbf{mapview} and \textbf{tmap}) to be installed - please keep an eye on our \href{https://github.com/TWIAV/Spatial_Analysis_in_R_with_Open_Geodata}{GitHub repository}.
  \item No specific prior experience (with R) is required for this introductory session, though some basic experience in one or more of the following fields is certainly helpful: Data Science, Programming/Software Development and/or GIS.
\end{itemize}

\end{multicols}


[^1]: https://www.linkedin.com/in/ejhpolle/
[^2]: https://www.linkedin.com/in/willytadema/

##Course material available on GitHub

The URL of this repository: [https://github.com/TWIAV/Spatial_Analysis_in_R_with_Open_Geodata](https://github.com/TWIAV/Spatial_Analysis_in_R_with_Open_Geodata)

 \begin{figure}[b]
\includegraphics{Images/Frontpage.png}
\end{figure}

\newpage
\newpage
\pdfbookmark{\contentsname}{toc}
\hypersetup{linkcolor=black}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\tableofcontents
\hypersetup{linkcolor=blue}
\newpage

# Managing Geospatial Vector Data in R {-}  

# The package `sf` (Simple Features for R) \label{simple_features}

To manage spatial data in R in this manual we will be using the library `sf`, a relativle new addition to the R universe. The package was [released on CRAN in January 2017](https://www.r-consortium.org/blog/2017/01/03/simple-features-now-on-cran).

This package provides support for simple features, which is a standardized way to encode spatial vector data.

`sf` links directly to three important geospatial libraries, to unlock their power for use in R:

* GDAL for reading and writing data
* GEOS for geometrical operations
* Proj.4 for projection conversions and datum transformations.

**The package `sf` on CRAN:**  
[https://cran.r-project.org/package=sf](https://cran.r-project.org/package=sf)


If the package is not yet installed, you can install it with the following command:

```{r install_sf, eval = FALSE}
install.packages("sf")
```

To be able to manage your spatial data you will first have to load the package `sf`:

```{r library_sf}
library(sf)
```

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{The real geospatial powers behind \lstinline{sf}}]
\begin{itemize}
\item GDAL: the \textbf{Geospatial Data Abstraction Library} is a translator library for raster and vector geospatial data formats. Website: \url{http://www.gdal.org/}
\item GEOS: the \textbf{Geometry Engine, Open Source} contains the complete functionality of the OpenGIS Simple Features for SQL spatial predicate functions and spatial operators. Website: \url{https://trac.osgeo.org/geos}
\item Proj.4: \textbf{PROJ} is a generic coordinate transformation software, that transforms coordinates from one coordinate reference system (CRS) to another. This includes cartographic projections as well as geodetic transformations. Website: \url{http://proj4.org/}
\end{itemize}
\end{tcolorbox}

\newpage

## `st_as_sf()`: converting a `data.frame` into an `sf` object

In this first exercise we will try to get some Dutch airports on the map. Let's start with defining some variables:

```{r}
ap <- "Lelystad Airport"
cd <- "LEY"
class(cd)
lat <- 52.460278
lon <- 5.527222
class(lon)
```

Now we will combine these variables into a `data.frame`:

```{r}
airport <- data.frame(ap, cd, lat, lon, stringsAsFactors = FALSE)
```

Wait, we have found a CSV file with some more airports:

```{r}
NL_Airports <- 
  read.csv("http://www.twiav.nl/files/NL_Airports.csv", stringsAsFactors = FALSE)

NL_Airports
```

Let's add our single airport to this dataset:

```{r}
NL_Airports <- rbind(NL_Airports, airport) 
# Oh yeah, of course: this will generate an error...
```

We can easily solve this issue setting the names equal to eachother:
```{r}
names(airport) <- names(NL_Airports)
```

Now we can `rbind` the two data.frames:

```{r}
NL_Airports <- rbind(NL_Airports, airport)
NL_Airports
class(NL_Airports)
```

Now, how do we convert this `data.frame` into an `sf` object? We have got information on the latitude and the longitude of our airports, and we can use this to create geometry, in this case: points.

Please note: the Coordinate Reference System (CRS) of these lat\\lon coordinates is WGS84, which has been given the EPSG code 4326. In chapter \ref{ChapCRS} we will have a closer look at spatial reference systems.

Now we can use the function `st_as_sf()` to convert our `data.frame` - with `longitude` first:

```{r}
library(sf)

NL_Airports <- st_as_sf(NL_Airports, coords = c("longitude","latitude"), crs = 4326)

class(NL_Airports)
```

Our dataset is now both a `data.frame` and an `sf` object, with all the conveniences which come with this dual status!

\begin{figure}[H]
\centering
\caption{Our first \textbf{sf} object in the \textbf{RStudio Environment pane}}
\vspace{5pt}
\includegraphics[width=172mm]{Images/sf_object_in_environment.png}
\label{sf_object_in_environment}
\end{figure}

\newpage

And when we plot the data to the screen we can see the sheer beauty of our **Simple Features**: the original columns containing the lat\\lon information have been used to create a column containing points. And this geometry column is stored next to the attribute data, in the very same `data.frame`. Wonderful, isn't it? 

```{r}
NL_Airports
```

And we can plot our airports like this:

```{r}
plot(st_geometry(NL_Airports), main = "Airports in the Netherlands", pch = 17)
```



\newpage

## Interactive Viewing of Spatial Data in R

In the previous paragraph we have seen a static map plotted on the **Plots** pane. But as a real Data Scientist we also want to be able to explore our data on an interactive map. Until a few years ago this would have meant switching back and forth between a desktop GIS and R. But in recent years some new packages have been developed to enable interactive map viewing in R.

In this paragraph we will look at two of these.

### The package `mapview`

**The package `mapview` on CRAN:**  
[https://cran.r-project.org/package=mapview](https://cran.r-project.org/package=mapview)

If the package is not yet installed, you can install it with the following command:

```{r, eval = FALSE}
install.packages("mapview")
```

To be able to view your spatial data interactively you will first have to load the package `mapview`:

```{r}
library(mapview)
```

Now we can open up an interactive map using a the function `mapview()` (see Figure \ref{Interactive_mapping_mapview1}):

```{r eval = FALSE}
mapview(NL_Airports, color = "red", col.regions = "orange", alpha.regions = 1, label = NL_Airports$airport)
```

\begin{figure}[H]
\centering
\caption{An interactive map can be opened up using a simple mapview() statement}
\vspace{5pt}
\includegraphics{Images/Interactive_mapping_mapview.png}
\label{Interactive_mapping_mapview1}
\end{figure}


\newpage

### The package `tmap`

**The package `tmap` on CRAN:**  
[https://cran.r-project.org/package=tmap](https://cran.r-project.org/package=tmap)

If the package is not yet installed, you can install it with the following command:

```{r, eval = FALSE}
install.packages("tmap")
```

To be able to view your spatial data interactively you will first have to load the package `tmap`:

```{r}
library(tmap)
```

\newpage

## `st_write()`

To share the spatial data in a simple features object with non-R users we have to write it to a spatial database or file format with the function `st_write()`

### Export data to an ESRI Shapefile \label{esrishape}

A well-known format to write to is the ESRI Shapefile format (see Figure \ref{output_shapefile1}):

```{r}
st_write(NL_Airports, "NL_Airports.shp")
```

```{r echo = FALSE}
# Delete the output files to keep the repository clean
unlink("NL_Airports.*")
```

\begin{figure}[H]
\centering
\caption{The shapefile with the corresponding files are written to your Working Directory}
\vspace{5pt}
\includegraphics{Images/output_shapefile.png}
\label{output_shapefile1}
\end{figure}

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{The ESRI Shapefile}]
The shapefile format is a well-known and still rather popular geospatial vector data format for Geographic Information System (GIS) software. It spatially describes vector features - points, lines, and polygons - with attribute data attached. The shapefile is a 'classic' GIS file format in the sense that it stores geometry and attribute data in separate files (in this chapter we will discover that a single shapefile in reality consists of multiple files) as opposed to more modern spatial database or file formats, where geometry and attributes are stored together in a single table or file.\\
\\
The shapefile format has been developed by \href{https://www.esri.com/}{Esri} and over the years has become a \textit{de facto} standard for data interoperability among Esri and other GIS software products.
\end{tcolorbox}

\newpage  

### Export data to a GeoJSON file 

Another format we may use is GeoJSON (see Figure \ref{nl_airports_geojson_in_npp1}):

```{r}
st_write(NL_Airports, "NL_Airports.geojson")
```

```{r echo = FALSE}
# Delete the output files to keep the repository clean
unlink("NL_Airports.geojson")
```

\begin{figure}[H]
\centering
\caption{The GeoJSON file can be viewed in a text editor}
\vspace{5pt}
\includegraphics{Images/nl_airports_geojson_in_npp.png}
\label{nl_airports_geojson_in_npp1}
\end{figure}



\newpage

# Accessing Geospatial Vector Data over the Internet {-}

# Downloadable shapefiles

\begin{multicols}{2}

In this chapter we will learn how to download, to unzip and to load shapefiles \textbf{using R}. We have seen the ESRI Shapefile before in paragraph \ref{esrishape}. There will be no need to use your web browser, your file explorer or a zip utility - the whole process can be completed using just a few lines of R code.

It is not uncommon for public organizations - including national statistical institutes - to distribute geographic information in this shapefile format. So, you will find shapefiles on the \href{https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/geografische-data}{page with geographic data of the Dutch Office for National Statistics (CBS)} and also on the \href{http://data.statistik.gv.at/web/catalog.jsp}{STATISTIK AUSTRIA open.data Portal}.

\includegraphics{Images/logo_statistik.png}

\end{multicols}

## Download and unzip the shapefile

```{r results = 'hide'}
# Store the URL to the file to download in a variable
URL2zip <- "http://data.statistik.gv.at/data/OGDEXT_GEM_1_STATISTIK_AUSTRIA_20180101.zip"

# Create a temporary file 
zip_file <- tempfile(fileext = ".zip")

# Download the file
download.file(URL2zip, destfile = zip_file, mode = "wb")

# Create a subfolder in your working directory to store the unzipped data
dir.create("./Data", showWarnings = FALSE)

# Unzip the file
unzip(zip_file, exdir = "./Data")
      
# After unzipping you can delete (i.e. unlink) the file
unlink(zip_file)

# Remove variables you do not longer need
rm(URL2zip, zip_file)
```

\newpage

## Load the shapefile

```{r}
library(sf)
library(tmap)
AUSTRIA_GEM_20180101 <- st_read("./Data/STATISTIK_AUSTRIA_GEM_20180101.shp")
```

```{r}
qtm(AUSTRIA_GEM_20180101)
```


\newpage

# OGC Web Feature Service (WFS) \label{chapWFS}

## Introduction

\begin{multicols}{2}

In this chapter we will learn how \textbf{to use R as a client to access data using a Web Feature Service (WFS)}. WFS is a Data Access Standard which is defined and maintained by the Open Geospatial Consortium (OGC). The WFS Interface Standard defines a set of interfaces for accessing geographic information over the Internet. It offers the means to retrieve geographic features and their properties through a highly configurable interface and in a manner independent of the underlying data stores they publish.

The standard is used - both in the public and private sector and in the academic world - to publish vector geospatial datasets in a way that makes it easy for receiving organisations to conduct analysis on the data supplied.

In a short chapter like this it will not be possible to cover all ins and outs of the WFS Interface Standard. The main goal here is to get you up and running and to whetten your appetite for more. It might all look a bit technical and intimidating at first, but as soon as you have the syntax of your request right, you will be able to retrieve valuable spatial data in the blink of an eye.

\end{multicols}

## WFS Outputformat: GML vs. GeoJSON \label{gmlGEOJSON}

By default, a WFS returns data in Geography Markup Language (GML) which is written as eXtensible Markup Language (XML). However, many WFS services also offer the option to request the ouput in GeoJSON, a geospatial data interchange format based on JavaScript Object Notation (JSON).

By it's very nature, GML data is difficult to process, because - as with most XML based grammars - there are two parts to the grammar: the schema that describes the document and the instance document that contains the actual data.

On the contrary, the GeoJSON standard clearly defines several types of JSON objects and the manner in which they are combined to represent data about geographic features, their properties, and their spatial extents.

In general **GDAL**, the translator library behind the `sf` functions `st_read()` and `st_write` (see chapter \ref{simple_features}), gives better results with GeoJSON as opposed to GML.

So, in the exercises in this manual, when accessing a WFS service to retrieve data, we will always add the parameter `outputFormat=application/json`.

## Access a WFS service: `request=GetCapabilities` \label{GCnl}

In general, organisations publishing data using WFS, will provide you with a URL to their WFS server which also contains a `GetCapabilities` request. Once you know the **capabilities** of the service (i.e. once you know what is on offer) you can start building your own request to the server.

In the exercises below we will use data on the municipal division in the Netherlands. These data are offered by the host of this conference, the Dutch statistical office, through [Publieke Dienstverlening op de Kaart](https://www.pdok.nl/) (PDOK) and the [Nationaal Georegister](http://www.nationaalgeoregister.nl/) (NGR).

Some background information on the dataset used, can be found here: [Dataset: CBS Gebiedsindelingen](https://www.pdok.nl/introductie?articleid=1951759).

The actual URL with the `GetCapabilities` reguest is:  
[https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetCapabilities](https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetCapabilities)

When you click this link the server response will be shown in your browser in XML format.

We are not going to study this XML response line-by-line. Not now. But please leave the web page open in your browser for later reference.

## Retrieve data from a WFS service: `request=GetFeature` \label{GetFeaturePar}

### `request=GetFeature` - an example from the Netherlands \label{GFnl}

A WFS server responding to a *GetFeature* request returns a collection of geographic feature instances filtered according to a criteria set by the requesting client. We will start with a simple request to download a full feature collection without any constraints to filter the content by. The GetFeature request queries the server with a set of parameters, which are concatenated to the URL with an *ampersand* (&).

In the script below we use the `httr` package. This allows us to store the different parameters of our request in a list, only to build the full URL at the end. We do so for readability reasons and to allow for easy modification of our request at a later stage if necessary. And the function `build_url()` will return a properly encoded URL.

A WFS service can offer one or more feature collections, see the `<FeatureTypeList>` section in the XML response to the `GetCapabilities` request. The service we are accessing here offers quite some feature collections, i.e. multiple regional divisions for the years 1995 up to the current year. What we are interested in here, is the municipal division for the year 2017. After some browsing through the XML response, we have found a `<FeatureType>` with the `<Name>` **cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd**. And that's the value we are giving to the `typename` parameter. (This `typenames` parameter determines the collection of feature instances to return.)

Also, do not forget to add a parameter to ask for output in GeoJSON format (as discussed in paragraph \ref{gmlGEOJSON}).

This is the script to populate the full request:

```{r}
# NL: Example with Dutch data
library(sf)
library(tmap)
library(httr)
library(data.table)

url <- list(hostname = "geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs",
            scheme = "https",
            query = list(service = "WFS",
                         version = "2.0.0",
                         request = "GetFeature",
                         typename = 
                           "cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd",
                         outputFormat = "application/json")) %>% 
       setattr("class","url")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=cbsgebiedsindelingen%3Acbs_gemeente_2017_gegeneraliseerd&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

Now we want to feed this response directly into R, like this:

```{r}
NL_Municipalities2017 <- st_read(request)
```

\newpage
Have a look at the result:

```{r}
qtm(NL_Municipalities2017)
head(NL_Municipalities2017)
```
\newpage

### `request=GetFeature` - an example from Finland


In this paragraph we will retrieve data from [Tilastokeskus (Statistics Finland)](http://www.stat.fi/). In the English section of their website we found a nice dataset: [Population by municipality-based units](http://www.stat.fi/org/avoindata/paikkatietoaineistot/vaesto_tilastointialueittain_en.html).


\begin{multicols}{2}
And from this map it is especially the layer \textbf{Population 2017 by municipalities 2018 (kunta\_vaki2017)} we would like to investigate.

We will follow the same steps as in the Dutch example above. For an explanation of the steps taken, please refer to paragraph \ref{GCnl} and \ref{GFnl}.

\includegraphics{Images/tklogo_fi.png}

\end{multicols}

The URL with `GetCapabilities` request is:  
[http://geo.stat.fi/geoserver/vaestoalue/wfs?request=GetCapabilities](http://geo.stat.fi/geoserver/vaestoalue/wfs?request=GetCapabilities)

In this XML response - in the `<FeatureTypeList>` section - we have found a `<FeatureType>` with the `<Name>` **vaestoalue:kunta_vaki2017**. That's the value we are giving to the `typename` parameter.

The script to build the full URL is similar to the one before - the only parameters we have modified are the hostname and the typename:

```{r}
# FI: Example with Finnish data
library(sf)
library(tmap)
library(httr)
library(data.table)

url <- list(hostname = "geo.stat.fi/geoserver/vaestoalue/wfs",
            scheme = "https",
            query = list(service = "WFS",
                         version = "2.0.0",
                         request = "GetFeature",
                         typename = "vaestoalue:kunta_vaki2017",
                         outputFormat = "application/json")) %>% 
       setattr("class","url")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://geo.stat.fi/geoserver/vaestoalue/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=vaestoalue%3Akunta_vaki2017&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

Now we want to feed this response directly into R, like this:

```{r}
FI_Municipalities2018_Pop2017 <- st_read(request)
```
\newpage

Have a look at the result:

```{r}
qtm(FI_Municipalities2018_Pop2017)
```

Optionally, you can have a look at the attribute data in the **Data Viewer**: `View(FI_Municipalities2018_Pop2017)`

\begin{multicols}{2}

With some basic knowledge of the Finnish language you should now be able to calculate that Finland had more or less 5.5 million inhabitants in 2017 - of which more or less 50 percent are males, and the other half females:

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=Finnish for Data Scientists: some useful words]
\begin{itemize}
\item \textbf{Miehet} = Men
\item \textbf{Naiset} = Women
\item \textbf{Nimi} = Name
\item \textbf{Vaesto} = Population
\end{itemize}
\end{tcolorbox}

\end{multicols}

```{r}
sum(FI_Municipalities2018_Pop2017$vaesto)
sum(FI_Municipalities2018_Pop2017$miehet)
sum(FI_Municipalities2018_Pop2017$naiset)
```

\newpage

## Get a description of a dataset: `request=DescribeFeatureType` \label{parDescFeatType}

In the previous exercises we immediately executed `GetFeature` requests to WFS services - mainly because the trainer told us to do so - without actually knowing anything about these datasets.

To get a description of the dataset, you can execute a `DescribeFeatureType` request first. This returns a description - in XML format - of the structure, including properties, of the feature type specified in the request.

For the Dutch municipalities dataset this request would look like this:
[https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd](https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd)

And for the Finnish dataset like this:
[http://geo.stat.fi/geoserver/vaestoalue/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=vaestoalue:kunta_vaki2017](http://geo.stat.fi/geoserver/vaestoalue/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=vaestoalue:kunta_vaki2017)

## Additional parameters to the `GetFeature` request

Additional parameters can be added to a `GetFeature` request to further filter or convert the response from the WFS.

To include additional parameters to a request, simply add an *ampersand* (&) at the end of the URL, then add the name of the parameter, an equal sign (=) and the value to assign to the parameter. Of course we will not do this manually; we will use the `httr` package to build this URL with additional parameters for us.

Below we will discuss a few of the parameters available.

### Limit the number of records returned: the `count` parameter

To have a look at the structure of a dataset - before retrieving the full dataset - you can choose to limit the number of records returned with the `count` parameter.

The `GetFeature` request below limits the number of Finnish municipalities returned to only 5:

```{r}
# FI: Example with Finnish data
library(httr)
library(data.table)

url <- list(hostname = "geo.stat.fi/geoserver/vaestoalue/wfs",
            scheme = "https",
            query = list(service = "WFS",
                         version = "2.0.0",
                         request = "GetFeature",
                         typename = "vaestoalue:kunta_vaki2017",
                         count = 5,
                         outputFormat = "application/json")) %>% 
       setattr("class","url")
request <- build_url(url)
```
The variable `request` will now contain this [link](https://geo.stat.fi/geoserver/vaestoalue/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=vaestoalue%3Akunta_vaki2017&count=5&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

\newpage

### Limit the number of columns returned: the `PropertyName` parameter

To restrict a `GetFeature` request by attribute rather than feature, use the `PropertyName` parameter. You can specify a single attribute, or multiple attributes separated by commas.

The `GetFeature` request below only returns the geometry and the columns *nimi* (Name) and *vaesto* (total population) for the Finnish municipalities:

```{r}
# FI: Example with Finnish data
library(httr)
library(data.table)

url <- list(hostname = "geo.stat.fi/geoserver/vaestoalue/wfs",
            scheme = "https",
            query = list(service = "WFS",
                         version = "2.0.0",
                         request = "GetFeature",
                         typename = "vaestoalue:kunta_vaki2017",
                         propertyname = "geom,nimi,vaesto",
                         outputFormat = "application/json")) %>% 
       setattr("class","url")
request <- build_url(url)
```
The variable `request` will now contain this [link](https://geo.stat.fi/geoserver/vaestoalue/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=vaestoalue%3Akunta_vaki2017&propertyname=geom%2Cnimi%2Cvaesto&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

## Further reading

The OGC offers an official tutorial module - [OGC E-learning](http://cite.opengeospatial.org/pub/cite/files/edu/index.html) - covering it's activities and the several standards it maintains. The [specific chapter on WFS](http://cite.opengeospatial.org/pub/cite/files/edu/wfs/text/basic-index.html) can be found [here](http://cite.opengeospatial.org/pub/cite/files/edu/wfs/text/basic-index.html).

\newpage

\begin{figure}[H]
\centering
\caption{On the overview page metadata about the \textbf{Feature Layer} \textit{USA States (Generalized)} is presented}
\vspace{5pt}
\includegraphics{Images/Feature_Layer_USA_States_Metadata.png}
\label{USA_States_AGOL_Metadata}
\end{figure}
\begin{figure}[H]
\centering
\caption{Here the \textbf{Feature Layer} \textit{USA States (Generalized)} is shown in the \textbf{ArcGIS Online Map Viewer}}
\vspace{5pt}
\includegraphics{Images/Feature_Layer_USA_States_Map_Viewer.png}
\label{USA_States_AGOL_MapViewer}
\end{figure}

\newpage

# ArcGIS REST Service

## Introduction

\begin{multicols}{2}

In this chapter we will learn how \textbf{to use R as a client to access data using an ArcGIS REST Service}. We will be accessing ArcGIS Server to retrieve Feature Layers using the \href{https://developers.arcgis.com/rest/}{ArcGIS REST API}. This API is part of ArcGIS Online,  the "complete SaaS mapping platform" by the American GIS company \href{https://www.esri.com/en-us/home}{Esri}. And as part of this platform Esri has launched the \href{https://livingatlas.arcgis.com/en/}{ArcGIS Living Atlas of the World}\footnote{Some background information on the Living Atlas of the World can be found in this \href{http://www.esri.com/esri-news/arcuser/summer-2015/welcome-to-the-living-atlas-of-the-world}{article}, where Esri also invites YOU to \textit{"Become a user and a contibutor"}.}

Some datasets in this Living Atlas are Open Data. Some, but by no means all, as many datasets are only accessible via an ArcGIS Online organizational account. But in cases where the source data is already Open Data, Esri publishes the data under the same conditions.

\includegraphics{Images/living-atlas-logo-size-4.png}

\end{multicols}

## Exploring data in the ArcGIS Living Atlas of the World

After some browsing through the content of the Living Atlas we have found a nice (and open) dataset for this exercise: the **Feature Layer** *USA States (Generalized)*. An overview page with metadata for the dataset we have chosen can be found via this [link](https://www.arcgis.com/home/item.html?id=99fd67933e754a1181cc755146be21ca#overview) (see Figure \ref{USA_States_AGOL_Metadata}). 

Before actually importing the data into R, you can explore the dataset - both the geometry and the attribute data - in the [ArcGIS Online Map Viewer](https://www.arcgis.com/home/webmap/viewer.html?useExisting=1&layers=99fd67933e754a1181cc755146be21ca) (see Figure \ref{USA_States_AGOL_MapViewer}).

## Query the Feature Service

On the overview page we have found the Service URL pointing to the layer **USA_States_Generalized** in the the Services Directory (see Figure \ref{USA_States_AG_REST_Services_Dir}): 

[https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0)

One of the **Supported Operations** for this layer is **Query**. The *Query* page (see Figure \ref{USA_States_AG_REST_Services_Dir_Query}) offers an interface to construct the query: 

[https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query)

Let's try to build a request to retrieve the full dataset:

* The first parameter is `where`. It is obligatory and cannot be omitted. If there is no *where-clause* (i.e. if you want to extract the full dataset) you have to populate this parameter with the value `1=1` (which is - of course - always true...)
* To get all the attribute columns you have to give the parameter `outFields` the value `*`
* To retrieve the spatial data the parameter `returnGeomatry` should be set to `true`
* And the outputformat - `f` - is set to `geojson`


\newpage

\begin{figure}[H]
\centering
\caption{On this page the \textbf{Feature Layer} \textit{USA\_States\_Generalized} is presented in the \textbf{ArcGIS REST Services Directory}}
\vspace{5pt}
\includegraphics{Images/ArcGIS_REST_Services_Directory_USA_States_FeatureServer.png}
\label{USA_States_AG_REST_Services_Dir}
\end{figure}
\begin{figure}[H]
\centering
\caption{On this page you can enter parameters to query the \textbf{Feature Layer} \textit{USA\_States\_Generalized}}
\vspace{5pt}
\includegraphics{Images/ArcGIS_REST_Services_Directory_USA_States_FeatureServer_query.png}
\label{USA_States_AG_REST_Services_Dir_Query}
\end{figure}


\newpage

This is the script to populate the full request:

```{r}
# USA: An example with American data
library(sf)
library(tmap)
library(httr)
library(data.table)

url <- list(hostname = "services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services",
            scheme = "https",
            path = "USA_States_Generalized/FeatureServer/0/query",
            query = list(where = "1=1",
                         outFields = "*",
                         returnGeometry = "true",
                         f = "geojson")) %>% 
       setattr("class","url")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query?where=1%3D1&outFields=%2A&returnGeometry=true&f=geojson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

Now we want to feed this response directly into R, like this:

```{r}
USA_States_2017 <- st_read(request)
```

\newpage

Have a look at the result:

```{r}
qtm(USA_States_2017)
```



\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{Anchorage}]
"Hey Chel you know it's kinda funny\\
 Texas always seems so big\\
 But you know you're in the largest State in the Union\\
 When you're anchored down in Anchorage"\\
\\ 
 \textbf{Michelle Shocked} - Short Sharp Shocked - 1988
\end{tcolorbox}

\newpage

## Additional parameters to the `query` request

### Filter the records returned: specify the `where`-clause

To filter records you can specify a `where`-clause.

The `query` request below only returns those States which have more than 10 million inhabitants:

```{r}
# USA: An example with American data
library(httr)
library(data.table)

url <- list(hostname = "services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services",
            scheme = "https",
            path = "USA_States_Generalized/FeatureServer/0/query",
            query = list(where = "POPULATION>10000000",
                         outFields = "STATE_NAME,POPULATION",
                         returnGeometry = "true",
                         f = "geojson")) %>% 
       setattr("class","url")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query?where=POPULATION%3E10000000&outFields=STATE_NAME%2CPOPULATION&returnGeometry=true&f=geojson). (When you click this link the server response will be shown in your browser in GeoJSON format.)


### Limit the number of columns returned: the `outFields` parameter

If you do not need all attribute columns of a dataset in your analysis, you can limit the number of columns returned by specifying the `outFields` parameter. You can specify a single attribute, or multiple attributes separated by commas.

The `query` request below only returns the geometry and the columns STATE_NAME and POPULATION for the United States:

```{r}
# USA: An example with American data
library(httr)
library(data.table)

url <- list(hostname = "services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services",
            scheme = "https",
            path = "USA_States_Generalized/FeatureServer/0/query",
            query = list(where = "1=1",
                         outFields = "STATE_NAME,POPULATION",
                         returnGeometry = "true",
                         f = "geojson")) %>% 
       setattr("class","url")
request <- build_url(url)
```

The variable `request` will now contain this [link](https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_States_Generalized/FeatureServer/0/query?where=1%3D1&outFields=STATE_NAME%2CPOPULATION&returnGeometry=true&f=geojson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

\newpage

# Statistics Netherlands (CBS) StatLine databank as open data

\begin{multicols}{2}

All tables in the \href{https://opendata.cbs.nl/statline/portal.html?_la=en&_catalog=CBS}{Statistics Netherlands (CBS) StatLine databank} are available as open data. Since 2014 this databank has an open data web API based on the OData protocol (\url{https://www.odata.org/}).

Using web services, data can be retrieved, filtered and combined. In this way, Statistics Netherlands aims to promote the widespread use of its statistical data.

In this chapter we will load Dutch statistical data directly into R, using the package `cbsodataR`, the Statistics Netherlands (CBS) Open Data API Client for R.

\includegraphics{Images/cbs.png}

\end{multicols}

## the package `cbsodataR`

The package `cbsodataR` can be found on CRAN: [https://cran.r-project.org/package=cbsodataR](https://cran.r-project.org/package=cbsodataR)

```{r table_of_contents, echo = FALSE, results = 'hide'}
library(cbsodataR)

toc <- cbs_get_toc()
nrow(toc)
toc_nl <- cbs_get_toc(Language = "nl")
nrow(toc_nl)
toc_en <- cbs_get_toc(Language = "en")
nrow(toc_en)
```

The table of contents can be retrieved with the function `cbs_get_toc()`. As we browse through the table of contents we do see a total of `r nrow(toc)` entries. For most datasets all the metadata is in Dutch, but for some of them this information is also available in English.

When we split the TOC by language we do see that currently `r nrow(toc_en)` datasets are available with metadata in English. These 'English' datasets are actually copies of their Dutch equivalents. (Maybe overtime all tables will be available with a description in English?)

This is a wealth of information, and all these datasets can be directly loaded into R. In the next paragraph we will do some exercises with one particular table.

```{r ref.label = 'table_of_contents'}
```

\newpage

## Exercises with regional statistical data

### Preparing the data

The table we have chosen for the exercises in this paragraph is called 'Regionale kerncijfers Nederland', i.e. regional statistical data about the Netherlands.
The identifier of this table is **70072ned**. It is a huge table containing data at 5 regional levels - from the country as a whole down to the municipal level - from 1995 up to the current year.

The table can be retrieved with the function `cbs_get_data()`:

```{r eval = FALSE}
NL_Regional_Statistics2017 <- cbs_get_data('70072ned')
```

As we do not need the full table, we will limit the amount of data loaded.

Firstly, we are only interested in the year 2017. We want to analyse the total population by region for that particular year.

So, we come up with the following statement (we will explain the selection of columns below):

```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(data.table)
library(stringr)
```

```{r}
NL_Regional_Statistics2017 <- cbs_get_data('70072ned', Perioden = "2017JJ00") %>% 
  select(RegioS, Perioden, TotaleBevolking_1, Code_291, Naam_292, Code_293, Naam_294)
```

The first two columns - `RegioS` and `Perioden` - are categorical columns, i.e. they contain codes. The labels for these columns can be added with the function `cbs_add_label_columns()`:

```{r}
NL_Regional_Statistics2017 <- cbs_add_label_columns(NL_Regional_Statistics2017)
```

We will now rename the columns of our table for further use in the exercise:

```{r}
NL_Regional_Statistics2017 <- rename(NL_Regional_Statistics2017, 
                                     Code = RegioS, 
                                     Name = RegioS_label,
                                     Year = Perioden,
                                     Year_label = Perioden_label,
                                     Total_population = TotaleBevolking_1,
                                     Region_code = Code_291,
                                     Region_name = Naam_292,
                                     Province_code = Code_293,
                                     Province_name = Naam_294)
```
Each column has a label. To avoid issues later on in this exercise, we will remove these labels now:

```{r}
cols <- colnames(NL_Regional_Statistics2017)
for (c in cols) {
  attr(NL_Regional_Statistics2017[[c]], "label") <- NULL
}
```

Let's have a first look at the table:

```{r}
head(NL_Regional_Statistics2017)
```

In the first row we can see the total number of inhabitants in the Netherlands on January 1st, 2017, which is: **17,081,507**.

And wait, we notice something else: some columns seem to have a fixed width. For example, the columns `Code` and  `Region_name`:

```{r}
paste(NL_Regional_Statistics2017$Code[1])
paste(NL_Regional_Statistics2017$Region_name[6])
```
```{r}
my_string <- paste(NL_Regional_Statistics2017$Region_name[6])
nchar(my_string)
```
This is weird, isn't it? The string 'Noord-Nederland' - which is only 15 charaters in length - is filled out with trailing spaces to have a length of 50... This can't be good, can it? We want our strings to behave like strings with their proper length, not carrying around a trail of whitespaces.

Let's fix this issue with the function `trimws()` for the columns concerned:

```{r}
cols <- c('Code', 'Region_code', 'Region_name', 'Province_code', 'Province_name')
NL_Regional_Statistics2017[cols] <- 
  lapply(NL_Regional_Statistics2017[cols], function(x){as.factor(trimws(x))})
```

```{r}
head(NL_Regional_Statistics2017)
```

This looks much better!

\newpage

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{NUTS - Nomenclature des Unités Territoriales Statistiques}]
\href{http://ec.europa.eu/eurostat/web/main/home}{Eurostat} uses a set of geospatial classifications, but the heart of these is NUTS:  the \textbf{Nomenclature of Territorial Units for Statistics}.\\
This classification was set up by Eurostat at the beginning of the 1970s. It serves as a single, coherent system for dividing up the EU's territory in order to produce regional statistics for the European Union.\\
\\
Ther are three levels of NUTS regions:
\begin{itemize}
\item{NUTS 1: major socio-economic regions}
\item{NUTS 2: basic regions for the application of regional policies}
\item{NUTS 3: small regions for specific diagnoses}
\end{itemize}

\includegraphics{Images/Eurostat_logo.png}
More information on the NUTS classification can be found here: \url{http://ec.europa.eu/eurostat/web/nuts/background}\\
\\
Eurostat even created a nice video on the topic: \url{https://youtu.be/a4Y-hCQ-Klo}
\end{tcolorbox}

### Extracting NUTS 1 and NUTS 2 regions

Now that we have prepared our data, we can extract the data about NUTS 1 and NUTs 2 respectively. At NUTS 1 level the Netherlands is divided into 4 regions ('Landsdelen') and at NUTS 2 level into 12 provinces:

```{r}
NL_Regions2017_data <- filter(NL_Regional_Statistics2017, Code %like% "LD")
NL_Provinces2017_data <- filter(NL_Regional_Statistics2017, Code %like% "PV")
```

Please note: these new data frames inherit factor levels from the original data frame from which they are created. Use the **RStudio Environment pane**, to confirm that - for example - the `Name` column of the `NL_Provinces2017_data` table is now a Factor with 770 levels, whereas there are only 12 provinces. So you have to drop unused levels before you can use the new subsets for further analysis:

```{r}
# Make sure to drop unused levels from the factors in those new data.frames
NL_Regions2017_data <- droplevels(NL_Regions2017_data)
NL_Provinces2017_data <- droplevels(NL_Provinces2017_data)
```

\newpage

```{r}
NL_Provinces2017_data %>% 
  select(Code, Name, Total_population, Region_code, Region_name) %>% as.data.frame()
```

We have noticed that the province names in the column `Name` all have the suffix *(PV)*. Let's get rid of this suffix, while making sure that the column remains a factor:

```{r}
NL_Provinces2017_data$Name <- 
  as.factor(str_replace(NL_Provinces2017_data$Name," \\(PV\\)", ""))
```

Likewise, the region names in the column `Name` all have the suffix *(LD)*. We will also remove that:

````{r}
NL_Regions2017_data %>% select(Code, Name, Total_population) %>% as.data.frame()
NL_Regions2017_data$Name <- 
  as.factor(str_replace(NL_Regions2017_data$Name," \\(LD\\)", ""))
```

\newpage

### `barplot()`

Let's investigate the spread of the Dutch population over the different regions by creating a barplot.

The most basic call to the `barplot()` function, is just to provide a column from a dataset and leave all the defaults:

```{r exbplot1, eval= FALSE}
barplot(NL_Provinces2017_data$Total_population) 
# Try this yourself - result not printed in this manual
```

This returns a very basic plot, with no labels and no title, so not very useful.

Use `?barplot()` to see what arguments we can use. After some trial and error we might come to a statement like this:

```{r exbplot2}
barplot(NL_Provinces2017_data$Total_population / 1000000, 
        names = NL_Provinces2017_data$Name, 
        las = 2, cex.axis = .6, cex.names = .6, 
        cex.main = .8, cex.lab = .8, space = 0, 
        col = "lightblue", 
        ylab = "Inhabitants (* 1.000.000)", 
        main = "Number of Inhabitants by Province - The Netherlands - 2017")
```

Not bad at all, for a first attempt. Please note: the provinces are not presented in alphabetical order, but in their  *natural* order, from Groningen (PV20) in the North to Limburg (PV31) in the South.

The provinces at the NUTS 2 level are grouped into regions ('Landsdelen') at the NUTS 1 level. We do know for each province to which region it belongs. Wouldn't it be nice to reflect this division in the plot?

So, In the next plot we will color the bars by region - North, East, West, South - using the `col` argument.

We will need 4 different colors for this plot. In the statement below we use the `palette()` function to manipulate the color palette which is used when a `col=` has a numeric index. Using the `c()` function you create a vector with 4 distinct colors, which is passed as an argument into `palette()`:

```{r}
palette(c("royalblue3", "firebrick3", "darkolivegreen4", "goldenrod1"))
```

And now we can create the barplot like this - with the arguments for the legend provided as a `list`:

```{r}
barplot(NL_Provinces2017_data$Total_population / 1000000, 
        names = NL_Provinces2017_data$Name, 
        las = 2, cex.axis = .6, cex.names = .6, 
        cex.main = .8, border = "grey", 
        col = NL_Provinces2017_data$Region_code, 
        ylab = "Inhabitants ( * 1,000,000)", 
        main = "Number of Inhabitants by Province - The Netherlands - 2017", 
        legend.text = unique(NL_Provinces2017_data$Region_name), 
        args.legend = list(x = 'topleft', 
                           bty = 'n', 
                           fill = unique(NL_Provinces2017_data$Region_name), 
                           border = 'grey'))
```


\newpage

### `pie()`

Based on the plots in the previous sections we might think that almost half the Dutch population lives in the westernmost region.
  
To confirm this hypothesis we will create a pie chart with the population by region.

This basic statement will create a simple pie chart:
```{r}
pie(NL_Regions2017_data$Total_population, labels = NL_Regions2017_data$Name)
```
And yes, almost half of the Dutch population lives in West-Nederland.

\newpage

The pie chart on the previous page already confirmed what we wanted to know. But of course - with a little extra effort - we can improve the quality of the graph by adding a title, calculating the actual percentages and using the same colors for the regions as we did in the barplot:

```{r}
pct <- paste0(round(NL_Regions2017_data$Total_population / 
                      sum(NL_Regions2017_data$Total_population) * 100, 1), "%")
lbls <- paste(NL_Regions2017_data$Name, "\n", pct)
palette(c("royalblue3", "firebrick3", "darkolivegreen4", "goldenrod1"))
pie(NL_Regions2017_data$Total_population, labels = lbls, clockwise = TRUE,
    cex = .8, col = NL_Regions2017_data$Name, border = "grey",
    main = "Percentage of Inhabitants by Region - The Netherlands - 2017")
```

```{r echo = FALSE, results = 'hide', message = FALSE}
palette("default")
```

\newpage

### Extracting municipal data


Now we will extract the municipalities from `NL_Regional_Statistics2017`.

```{r}
NL_Municipalities2017_data <- filter(NL_Regional_Statistics2017, Code %like% "GM")
nrow(NL_Municipalities2017_data)
```
Are there `r nrow(NL_Municipalities2017_data)` municipalities in the Netherlands? No, not anymore!
There used to be much more: in 1850 there were more or less 1200 municipalities. But today there are much less: the current (2018) amount is 380. The Netherlands has a long standing tradition of grouping smaller municipalities into larger ones. This is a slow process, but each year on the 1^st of January the number of municipalities diminishes again.

We are looking at 2017, and in that year there were still 388 municipalities.

The table we have accessed contains data from 1995 up to today. And apparently in that year there were still 713 municipalities.

We can filter out the municipalities that do no longer exist by removing the ones without data. Here we use the column `Total_population` to check for that:

```{r}
NL_Municipalities2017_data <- filter(NL_Municipalities2017_data, Total_population != "")
nrow(NL_Municipalities2017_data)
```

And of course we should also drop unused factor levels:
```{r}
NL_Municipalities2017_data <- droplevels(NL_Municipalities2017_data)
```

\newpage

### Merging sf and data.frame objects

Now we want to plot a map with the regional subdivision. To be able to do so we have to merge our data frame with a dataset containing geometry.

We will access a WFS server to download the municipal geometry - for the year 2017! - as we have done in paragraph \ref{GFnl}:

```{r}
# NL: Example with Dutch data
library(sf)
library(tmap)
library(httr)
library(data.table)
library(dplyr)

url <- list(hostname = "geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs",
            scheme = "https",
            query = list(service = "WFS",
                         version = "2.0.0",
                         request = "GetFeature",
                         typename = 
                           "cbsgebiedsindelingen:cbs_gemeente_2017_gegeneraliseerd",
                         outputFormat = "application/json")) %>% 
       setattr("class","url")
request <- build_url(url)

NL_Municipalities2017 <- st_read(request)
```

As we do not need all attribute columns, we will select only the relevant ones:

```{r}
NL_Municipalities2017 <- select(NL_Municipalities2017, statcode, statnaam)
```

Please note: in the statement above we only select 2 columns, but the resulting object does also contain the `geometry`column. This is due to the *sticky* character of this `geometry` column. When you select a subset of columns, the geometry will always be included, unless you explicitly drop it. In other words: a subset of an `sf` object will in itself also be an `sf` object.

Now we will rename the columns:

```{r}
NL_Municipalities2017 <- rename(NL_Municipalities2017, Code = statcode, Name = statnaam)
```

Both datasets do contain a column with municipal codes, so now you can merge them, like you would do with any two data frames:

```{r}
NL_Municipalities2017 <- 
  merge(NL_Municipalities2017, NL_Municipalities2017_data, by = "Code")

class(NL_Municipalities2017)
```
\newpage

### `qtm()`

And now we can plot our map with NUTS 1 regions, again using the same colors as before:

```{r}
qtm(shp = NL_Municipalities2017,
    title = "Regional subdivison",
    fill = "Region_name",
    fill.title = "The Netherlands - 2017\nRegion",
    fill.palette = 
      palette(c("royalblue3", "firebrick3", "darkolivegreen4", "goldenrod1")),
    borders = "grey",
    format = "NLD_wide")
```

\newpage

\tcbset{coltitle=black, colbacktitle=white, colframe=gray!75!black,colback=white,nobeforeafter}
\begin{tcolorbox}[enlarge by=5mm, hyphenationfix, title=\Large{Factors in R}]
Conceptually, factors are variables in R which can only contain a pre-defined set of values, known as levels. Factors are often referred to as \textit{categorical variables}, which can either be \textbf{ordered} (e.g. 'low', 'medium', 'high') or \textbf{unordered} (e.g. 'male', 'female').\\
\\
One of the most important uses of factors is in statistical analysis and plotting. Storing categorical variables as factors insures that the statistical modeling functions will treat such data correctly. Factors are stored in R as a vector of integers, with a corresponding set of labels. These labels - one for each level - are used when the factor is displayed.\\
\\
When data are being loaded into R, for example with the function \textbf{read.csv()}, all columns containing text (character data) will be automatically converted to factors. While this is very useful in many cases, there might also be situations where you will want to treat your data as continuous variables.
\end{tcolorbox}

## Factor: group municipalities into categories \label{parFactor}

In a previous exercise we have learned that there is an ongoing process of municipal regrouping in the Netherlands. Let's investigate opportunities for future regroupings by maping municipalities by size.

We will start this exercise by dividing our municipalities in 3 groups: small, medium and large.

A small municipality has less than 50,000 inhabitants, a medium sized one has between 50,000 and 200,000, whereas a municipality with more than 200,000 inhabitants is considered to be large.


```{r ordered_factor}
NL_Municipalities2017 <- mutate(NL_Municipalities2017,
  Category = case_when(Total_population < 50000 ~ "Small population",
                       Total_population >= 50000 & 
                       Total_population < 200000 ~ "Medium population",
                       Total_population >= 200000 ~ "Large population"))
```

```{r ordered_factor2}
class(NL_Municipalities2017$Category)
```

Now we will create an ordered factor with 3 levels:

```{r}
population_levels <- c("Small population", "Medium population", "Large population")
NL_Municipalities2017 <- mutate(NL_Municipalities2017, 
  Category = factor(Category, levels = population_levels, ordered = TRUE))

class(NL_Municipalities2017$Category)
```
```{r change_categories, echo = FALSE, results = 'hide', message = FALSE}
population_levels <- c("Very small population", "Small population", "Medium population", "Large population")
NL_Municipalities2017 <- mutate(NL_Municipalities2017, 
  Category = case_when(Total_population < 20000 ~ "Very small population", TRUE ~ as.character(Category)) %>% 
    factor(levels = population_levels, ordered = TRUE))
```

\newpage

And now we can plot our map.

Please note: in the map printed below there are not 3 categories, but 4. We have added a category 'very small' for municipalities with less than 20,000 inhabitants.

Question:

What steps do you need to take to add a new factor level after factors have been defined?

As you can see there is still some potential to cluster very small municipalities together. But of course, this will be more a political than a scientific issue.


```{r tmapcategories}
qtm(shp = NL_Municipalities2017, title = "Municipalities by Population Size",
    fill = "Category", fill.title = "The Netherlands - 2017\nCategory", 
    borders = "grey", format = "NLD_wide")
```

\newpage

# Spatial Reference Systems \label{ChapCRS}

\begin{multicols}{2}

In this chapter we will have a quick look at an important topic when handling geospatial data, i.e the Coordinate Reference System (or Spatial Reference System), which defines how spatial elements relate to the surface of the Earth and which is used to project our geodata on a flat screen. We have ignored this subject more or less in the previous exercises, mainly because of the fact that the whole matter of projecting the data is handled very well in the `sf` package, or rather the `proj.4` software behind it. We just didn't have to think about it. Or wait, we did set a `crs` explicitly - using an `epsg` code - in the very first exercise this morning. Remember?

In general when you read in spatial data it comes with a proper CRS and if all your data is in this same projection, you really don't have to worry about all the geodesy behind it. But what if you want to combine data from different sources which appear to have different spatial reference systems?

Therefor we will shortly touch upon:

\begin{itemize}
\item{Geographic coordinate systems vs. Projected coordinate systems}
\item{the two main ways to describe a CRS in R: the `epsg` code and the `proj4string` definition}
\item{how to reproject your data - temporarily or permanently}
\end{itemize}

\end{multicols}

## Coordinate systems: Geographic vs. Projected

Geographic coordinate systems identify locations on the Earth's surface using longitude and latitude. Longitude is the distance East or West in angular distance, i.e. measured in degrees, from the Prime Meridian plane. Latitude is this distance North or South of the equatorial plane. And no, there is not just one 'Lat\\Lon coordinate system'. All geographic systems have to use the same reference, i.e. the Prime Meridian and the Equator, but the exact location on the Earth depends on several parameters like the ellipsoid and the datum used. So even if someone gives you data 'in degrees' you still have to find out which CRS to use. With a wrong CRS your data might end up centimeters or even several meters from the correct location. An important advantage of these geographic systems is their global reach - they can be used everywhere and are useful when dealing with international datasets. An important disadvantage is: distances cannot be easily measured in for example meters or kilometers in geographic CRSs.

Projected coordinate system have a much smaller reach, as these are based on Cartesian coordinates on an implicitly flat surface. They are only valid for a specific area - a country or even just a part of a country - where they have an origin, x and y axes, and a linear unit of measurement such as meters. We have seen several examples of data in such local, projected systems, e.g. EPSG:31287 for Austria, EPSG:3067 for Finland and EPSG:28992 for the Netherlands (but only onshore). These systems are ideal for dealing with data at a national or subnational level. In such a case you shuld really stick with the local CRS. 

## `epsg` codes and. `proj4string` definitions

As there are so many coordinate systems worldwide it is good to be clear about what system you use in your analysis. There are two main ways to describe a CRS in R: the `epsg` code and the `proj4string` definition. If you get your spatial data from a proper source, the `st_read()` function will return both, as we have seen in the exercises before.

The `epsg` code gets it name from the - no longer existing - *European Petroleum Survey Group*. The set of EPSG definitions is currently maintained by the  International Association of Oil & Gas Producers (IOGP). Apparently location is very important in the fossil fuel industry.

An `epsg` code refers to only one, well-defined coordinate reference system. This coding system is widely adopted by the geospatial community, so if you get XY data with an existing `epsg` code you can easily plot your data on the map, in R or in any other GIS system for that matter.

A `proj4string` definition contains different parameters such as the projection type, the datum and the ellipsoid. It allows you to specify your own projection, or to modfy an existing one. But in general it would be advisable to refer to the data provider if there is something wrong with the projection of your data.

\newpage

## Reprojecting data

\newpage

# References {-}

* **Lovelace, Robin, Jakub Nowosad & Jannes Muenchow** (Forthcoming). *Geocomputation with R*. CRC Press. Online version: [https://geocompr.robinlovelace.net/](https://geocompr.robinlovelace.net/)
* **Pebesma, Edzer** (January 3, 2017). *Simple Features Now on CRAN*. Blog. Retreived from: [https://www.r-consortium.org/blog/2017/01/03/simple-features-now-on-cran](https://www.r-consortium.org/blog/2017/01/03/simple-features-now-on-cran)


\newpage

# Appendix A: List of abbreviations used {-}

Abbreviation | Meaning
-------------|----------
API | application programming interface
CRAN | Comprehensive R Archive Network
GIS | Geographic Information System
GML | Geography Markup Language
IOGP | International Association of Oil & Gas Producers
JSON | JavaScript Object Notation
OGC | Open Geospatial Consortium
REST | Representational State Transfer
SaaS | software as a service
WFS | Web Feature Service
XML | eXtensible Markup Language

\newpage

# Appendix B: Additional exercises {-}

## OGC Web Feature Service (WFS) {-}

For background information on the steps taken in the exercise below, please refer to chapter \ref{chapWFS}

### `request=GetFeature` - another example from the Netherlands {-}

In this additional exercise we will access a WFS service offering information about *bevolkingskernen* (human settlements) in the Netherlands in 2011. More information on this dataset can be found here: [Dataset: CBS Bevolkingskernen](https://www.pdok.nl/introductie?articleid=1951745).

The URL with `GetCapabilities` request is:  
[https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs?request=GetCapabilities](https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs?request=GetCapabilities)

In the `<FeatureTypeList>` section of this XML response we can see that this service only offers one `<FeatureType>`, i.e. one dataset. This dataset has the `<Name>` **bevolkingskernen2011:cbsbevolkingskernen2011**. That's the value we are giving to the `typename` parameter in our `GetFeature` request.


The URL with `DescribeFeatureType` request is:  
[https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=bevolkingskernen2011:cbsbevolkingskernen2011](https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs?service=WFS&version=2.0.0&request=DescribeFeatureType&typename=bevolkingskernen2011:cbsbevolkingskernen2011)

This dataset does contain quite some attribute columns!

This is the script to build the full `GetFeature` request:

```{r}
# NL: Human Settlement Analysis
library(sf)
library(tmap)
library(httr)
library(data.table)

url <- list(hostname = "geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs",
            scheme = "https",
            query = list(service = "WFS",
                         version = "2.0.0",
                         request = "GetFeature",
                         typename = "bevolkingskernen2011:cbsbevolkingskernen2011",
                         outputFormat = "application/json")) %>% 
       setattr("class","url")
request <- build_url(url)
```
The variable `request` will now contain this [link](https://geodata.nationaalgeoregister.nl/bevolkingskernen2011/wfs/?service=WFS&version=2.0.0&request=GetFeature&typename=bevolkingskernen2011%3Acbsbevolkingskernen2011&outputFormat=application%2Fjson). (When you click this link the server response will be shown in your browser in GeoJSON format.)

Now we want to feed this response directly into R, like this:

```{r}
NL_Human_Settlements2011 <- st_read(request)
```

```{r}
plot(st_geometry(NL_Human_Settlements2011), col = "orange", border = "red")
```

